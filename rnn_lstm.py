# -*- coding: utf-8 -*-
"""RNN_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HGmQX_hHbNdwx-XX37i5EQq7cv2Vcphn

# Pre processing
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import matplotlib.dates as mdates
import seaborn as sns
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

"""import data"""

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/datasets/Bhopal.csv'
df = pd.read_csv(file_path)

"""plot time series"""

ghi = df[(df['Year'] == 2019) & (df['Month'] < 6)]
mask_ghi = ghi['GHI'] > 150
ghi_f = ghi[mask_ghi]

# Combine year, month, day, and hour columns into a single datetime column
ghi_f['Date'] = pd.to_datetime(df[['Year', 'Month', 'Day', 'Hour']])

# Plot time series
plt.figure(figsize=(10, 6))
plt.plot(ghi_f['Date'], ghi_f['GHI'], color='blue')
plt.title('Time Series for Nov (GHI>150)')
plt.xlabel('Date')
plt.ylabel('GHI Value')
plt.grid(True)
plt.show()

"""train test split"""

#need to get a dataset in the form of a 2d array with 1 column
#check their shape to understand how many values are being run and tested in the model
trainset = ghi_f[['GHI']]
print(trainset.shape)

ghi2 = df[(df['Year'] == 2019) & (df['Month'] == 6)]
mask_ghi = ghi2['GHI'] > 150
ghi_f2 = ghi2[mask_ghi]

testset = ghi_f2[['GHI']]
print(testset.shape)

"""Scaling"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0,1))

#scaling values for faster model training
scaled_train = scaler.fit_transform(trainset)
scaled_test = scaler.fit_transform(testset)

print(*scaled_train[:5])
print(*scaled_test[:5])

"""creating sequences for PyTorch tensors"""

sequence_length = 30
X_train, y_train = [],[]
for i in range(len(scaled_train) - sequence_length):
  X_train.append(scaled_train[i:i+sequence_length])
  y_train.append(scaled_train[i+1:i+sequence_length+1])

X_train, y_train = np.array(X_train) , np.array(y_train)

#convert data in PyTorch tensors
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
X_train.shape, y_train.shape

sequence_length = 10
X_test, y_test = [],[]
for i in range(len(scaled_test) - sequence_length):
  X_test.append(scaled_test[i:i+sequence_length])
  y_test.append(scaled_test[i+1:i+sequence_length+1])

X_test, y_test = np.array(X_test) , np.array(y_test)

X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32)
X_test.shape, y_test.shape

"""# LSTM Model"""

class LSTMModel(nn.Module):

  def __init__(self, input_size, hidden_size, num_layers):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.linear = nn.Linear(hidden_size, 1)

  def forward(self, x):
      out, _ = self.lstm(x)
      out = self.linear(out)
      return out

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

input_size = 1
num_layers = 2
hidden_size = 32
output_size = 1

# Define the model, loss function, and optimizer
model = LSTMModel(input_size, hidden_size, num_layers).to(device)

loss_fn = torch.nn.MSELoss(reduction='mean')

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
print(model)

batch_size = 16
# Create DataLoader for batch training
train_dataset = torch.utils.data.TensorDataset(X_train, y_train)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

test_dataset = torch.utils.data.TensorDataset(X_test, y_test)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

num_epochs = 50
train_hist =[]
test_hist =[]
# Training loop
for epoch in range(num_epochs):
    total_loss = 0.0

    # Training
    model.train()
    for batch_X, batch_y in train_loader:
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)
        predictions = model(batch_X)
        loss = loss_fn(predictions, batch_y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()


    average_loss = total_loss / len(train_loader)
    train_hist.append(average_loss)

    # Validation on test data
    model.eval()
    with torch.no_grad():
        total_test_loss = 0.0

        for batch_X_test, batch_y_test in test_loader:
            batch_X_test, batch_y_test = batch_X_test.to(device), batch_y_test.to(device)
            predictions_test = model(batch_X_test)
            test_loss = loss_fn(predictions_test, batch_y_test)

            total_test_loss += test_loss.item()

        # Calculate average test loss and accuracy
        average_test_loss = total_test_loss / len(test_loader)
        test_hist.append(average_test_loss)
    if (epoch+1)%10==0:
        print(f'Epoch [{epoch+1}/{num_epochs}] - Training Loss: {average_loss:.4f}, Test Loss: {average_test_loss:.4f}')

"""plotting the learning curve"""

x = np.linspace(1,num_epochs,num_epochs)
plt.plot(x,train_hist,scalex=True, label="Training loss")
plt.plot(x, test_hist, label="Test loss")
plt.legend()
plt.show()

"""#Plotting results"""



# Define the number of future time steps to forecast
num_forecast_steps = 50


last_sequence = X_test[-1,:,0]
historical_data = last_sequence.view(1,10,1)
print(historical_data.shape)


forecasted_values = []

# Use the trained model to forecast future values
with torch.no_grad():
	for _ in range(num_forecast_steps*2):
		historical_data_tensor = torch.as_tensor(historical_data).view(1, -1, 1).float().to(device)
		predicted_value = model(historical_data_tensor).cpu().numpy()[0, 0]
		forecasted_values.append(predicted_value[0])

		# Update the historical_data sequence by removing the oldest value and adding the predicted value
		historical_data = np.roll(historical_data, shift=-1)
		historical_data[-1] = predicted_value

# Generate the next 10 dates
last_date = testset.index[-1]
last_date_datetime = pd.to_datetime(last_date, unit='D')
future_dates = pd.date_range(start=last_date_datetime + pd.DateOffset(1), periods=30)
combined_index = testset.index.append(future_dates)

print(testset.head(3))

plt.rcParams['figure.figsize'] = [14, 4]

#Test data
plt.plot(testset.index[-100:-10], testset.GHI[-100:-10], label = "test_data", color = "b")
original_cases = scaler.inverse_transform(np.expand_dims(last_sequence, axis=0)).flatten()

plt.plot(testset.index[-10:], original_cases, label='actual values', color='green')

#Forecasted Values
forecasted_cases = scaler.inverse_transform(np.expand_dims(forecasted_values, axis=0)).flatten()
plt.plot(combined_index[-100:], forecasted_cases, label='forecasted values', color='red')

plt.xlabel('Time Step')
plt.ylabel('GHI Value')
plt.legend()
plt.title('LSTM Time Series Forecasting')
plt.grid(True)

